# 分词「Analysis」
- 也叫文本分析，是吧全文本转换一系列单词的过程。
- 通过Analyzer「分词器」来实现的
    - 可使用elasticsearch内置的分析器。
    - 可按需定制化分析器。
- 除了在数据写入时转换词条，匹配Query语句时也需要用相同的分析器对查询语句进行分析。

## Analyzer

### 组成
- Character Filters：针对原始文本处理，例如去除html。
- Tokenizer：按照规则切分为单词。
- Token Filter：将切分的单词进行加工（小写、删除、增加同义词）

## 内置分词器
- Standard analyzer

        1. 默认分词器
        2. 按词切分
        3. 小写处理
    - Tokenizer
        - Standard
    - Token Filter
        - Standard
        - Lower Case
        - Stop（默认关闭）

- Simple analyzer

        1. 按照非字母切分。
        2. 小写处理。
    - Tokenizer
        - Lower Case

- Whitespace analyzer

        1. 按照空格切分
    - Tokenizer
        - Whitespace
- Stop analyzer

        1. 按照非字母切分。
        2. 把 the、a、is等修饰性词语去掉。
    - Tokenizer
        - Lower Case
    - Token Filter
        - Stop
- Keyword analyzer

        1. 不做分词，作为term输出
    - Tokenizer
        - Keyword
- Pattern analyzer

        1. 通过正则表达式进行分词
        2. 默认是\W，非字符的符号进行分隔。
    - Tokenizer
        - Pattern
    - Token Filter
        - Lower Case
        - Stop
- Language analyzer
    - english
    - ICU analyzer
        - Character Filters
            - Normailzation
        - Tokenizer
            - ICU
        - Token Filter
            - Normailzation
            - Folding
            - Collation
            - Transform
    - IK analyzer
    - THULAC 清华大学自然语言处理和社会人文自然实验室